{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "#polyfill for plotting the data\n",
    "gui_env = ['TKAgg','GTKAgg','Qt4Agg','WXAgg']\n",
    "for gui in gui_env:\n",
    "    try:\n",
    "        matplotlib.use(gui,warn=False, force=True)\n",
    "        from matplotlib import pyplot as plt\n",
    "        break\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron: \n",
    "    def __init__(self, inputs, expected, lr): \n",
    "        \n",
    "        #rows: how many inputs are in the inputs array\n",
    "        rows = len(inputs)\n",
    "        #cols: how many datapoints are in each input array\n",
    "        cols = len(inputs[0]) + 1\n",
    "        \n",
    "        #lr: learning rate, plays a role in determining how \n",
    "        #fast the weights will changes\n",
    "        self.lr = lr\n",
    "        \n",
    "        #Each piece of data has an expected classificatoin\n",
    "        self.expected = expected\n",
    "        \n",
    "        #initialize an empty memory array, to add error rates for\n",
    "        #plotting performance over time\n",
    "        self.error_memory = []\n",
    "        \n",
    "        #Weights: how the perceptron processes data - used as variables\n",
    "        #in a classification function. Initialize all weights at zero\n",
    "        self.weights = np.zeros(cols)\n",
    "        \n",
    "        \n",
    "        self.inputs = np.zeros((rows, cols))\n",
    "        for i in range(len(inputs)):\n",
    "            #insert the bias at index 0 of each row, initialized at 1\n",
    "            self.inputs[i] = np.insert(inputs[i], 0, 1)\n",
    "    \n",
    "    #binary classifier (used to train)\n",
    "    #return one if greater than zero, \n",
    "    #otherwise return zero\n",
    "    def step(self, val):\n",
    "        if val > 0: \n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    #method: step gradient descent function (used to train)\n",
    "    #determines how bad the error was, and change the \n",
    "    #weights to try and be more accurate\n",
    "    def sgd(self, error, inputs): \n",
    "        delta_magnitude = self.lr * error\n",
    "        delta_weights = delta_magnitude * inputs\n",
    "        self.weights = self.weights + delta_weights\n",
    "    \n",
    "    \n",
    "    #method: train - adjusts the perceptrons weights \n",
    "    #arg: epochs - the number of times the dataset should be \n",
    "    #processed through the training protocol\n",
    "    def train(self, epochs): \n",
    "        for epic in range(epochs): \n",
    "            errors = 0\n",
    "            for i in range(len(self.inputs)): \n",
    "                #multiply each of the weights with the corresponding  \n",
    "                #inputs like a matrix dot multiply\n",
    "                #and sum the whole array into one value\n",
    "                predict = np.sum(self.weights * self.inputs[i])\n",
    "                \n",
    "                #activation is the classifier, in perceptrons it'll always \n",
    "                #either zero (inactive) or one (active)\n",
    "                activation = self.step(predict)\n",
    "                \n",
    "                #this binary classification is compared against the actual identity\n",
    "                #of the row of input data, which is being represented by zero or one\n",
    "                expected = self.expected[i]\n",
    "                err = expected - activation\n",
    "                if err != 0: \n",
    "                    errors += 1\n",
    "                    self.sgd(err,self.inputs[i])\n",
    "            \n",
    "            #on every 50th iteration through the data, remember the error rate: \n",
    "            if epic % (epochs/50) == 0: \n",
    "                error = (errors/len(self.inputs)) * 100\n",
    "                self.error_memory.append(error)\n",
    "                \n",
    "        print('Training complete! Updated weights: ', self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Updated weights:  [-603.   10.   10.]\n"
     ]
    }
   ],
   "source": [
    "#inputs:\n",
    "inputs = np.array([\n",
    "    [43,18],[62,16],[70,21],[71,20], #identities: 1\n",
    "    [12,12],[13,10],[25,4],[39,11] #identities: 0\n",
    "])\n",
    "#expected outputs:\n",
    "expected = np.array([1, 1, 1, 1, 0, 0, 0, 0])\n",
    "\n",
    "#learning rate doesn't matter that much\n",
    "lr = 1\n",
    "\n",
    "#instantiate and train the perceptron:\n",
    "perceptron = Perceptron(inputs, expected, lr)\n",
    "perceptron.train(600)\n",
    "\n",
    "#grab the stored error history from the training session:\n",
    "y_val = perceptron.error_memory\n",
    "x_val = range(len(y_val))\n",
    "\n",
    "#plot the error rate through time to demonstrate learning:\n",
    "plt.plot(x_val, y_val, '-o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
